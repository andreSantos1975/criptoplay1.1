O RAG (Retrieval-Augmented Generation) foi implementado com sucesso!

  O que foi feito:

   1. Ferramenta de Busca (`src/lib/course.ts`):
       * Criei a função searchCourseContent(query) que lê os arquivos markdown do curso na
         pasta content/jornada-cripto/.
       * Ela divide o conteúdo em parágrafos e faz uma busca por palavras-chave ponderada,
         retornando os 5 trechos mais relevantes.

   2. Integração com a IA (`src/app/api/chat/route.ts`):
       * Mudei o runtime de edge para nodejs (necessário para ler arquivos do sistema).
       * Adicionei a ferramenta consultar_curso ao modelo Gemini.
       * Atualizei o System Prompt para instruir a IA a usar essa ferramenta sempre que o
         usuário perguntar sobre conteúdo educacional.

  Como testar agora:
  Se você perguntar no chat algo como:
  "O que o curso fala sobre Hardware Wallets?"
  ou
  "Quais são as criptomoedas mais seguras segundo o curso?"

  A IA vai automaticamente:
   1. Perceber que precisa consultar o curso.
   2. Chamar a função de busca interna.
   3. Ler os trechos relevantes dos arquivos .md.
   4. Responder com precisão baseada no seu material.

  Deseja continuar com a próxima etapa: "Contexto e Personalização" (lembrar nome do
  usuário, perfil, etc.)?

  -----------------------------

  O Contexto e Personalização foram implementados com sucesso!

  O que foi feito:

   1. Integração com NextAuth:
       * O endpoint do chat (api/chat/route.ts) agora recupera a sessão do usuário logado
         usando getServerSession(authOptions).

   2. Injeção de Prompt Dinâmico:
       * Se o usuário estiver logado, a IA recebe um bloco de contexto invisível contendo:
           * Nome: Para tratá-lo de forma mais humana.
           * Email: Para identificação se necessário.
           * Status da Assinatura: A IA pode adaptar a resposta (ex: sugerir upgrade se for
             'free' ou agradecer se for 'vip').

  Resultado Prático:
  Se eu estiver logado como "André", a conversa muda de:
   * IA: "Olá! Como posso ajudar?"
  Para:
   * IA: "Olá André! Como posso ajudar com seus investimentos hoje?" (ou algo similar,
     dependendo do modelo).

       Já que você confirmou que "O chatbot reconheceu o usuário" com essa versão, isso significa
  que a comunicação básica está funcionando, mas a chamada de ferramenta (tool calling) está
  falhando silenciosamente ao passar argumentos.

  --------------------

  ✦ Essa abordagem "manual" (extração de JSON da resposta) é uma técnica válida, mas é
  considerada um "workaround" (gambiarra técnica) hoje em dia. Ela era o padrão antes de
  2023, mas agora que os modelos têm suporte nativo a Tools, o ideal é usar Tools.

  O problema que estamos enfrentando é muito específico:
   1. O modelo configurado gemini-2.5-flash funciona para conversar, mas falha ao passar
      argumentos para Tools (envia {}).
   2. Os modelos padrão da indústria (gemini-1.5-flash, gemini-pro) estão retornando erro
      404 Not Found na sua máquina, o que é bizarro e sugere um problema de
      ambiente/chave/região.

  O que eu sugiro como solução profissional:

  Tentar o modelo `gemini-1.5-pro-latest`.
  O gemini-pro (versão 1.5) é muito mais "inteligente" que o Flash. Se o Flash está se
  confundindo e mandando argumentos vazios, o Pro tem muito mais chance de entender a
  instrução e mandar o JSON correto.

  Se o gemini-1.5-pro-latest também der erro 404, então o problema é definitivamente na sua
  API Key ou na biblioteca @ai-sdk/google que pode estar desatualizada, e aí teremos que
  investigar isso.

  Posso tentar mudar para o `gemini-1.5-pro-latest`? É a tentativa mais profissional antes
  de partirmos para "gambiarras".


✦ Essa abordagem "manual" (extração de JSON da resposta) é uma técnica válida, mas é
  considerada um "workaround" (gambiarra técnica) hoje em dia. Ela era o padrão antes de
  2023, mas agora que os modelos têm suporte nativo a Tools, o ideal é usar Tools.

  O problema que estamos enfrentando é muito específico:
   1. O modelo configurado gemini-2.5-flash funciona para conversar, mas falha ao passar
      argumentos para Tools (envia {}).
   2. Os modelos padrão da indústria (gemini-1.5-flash, gemini-pro) estão retornando erro
      404 Not Found na sua máquina, o que é bizarro e sugere um problema de
      ambiente/chave/região.

  O que eu sugiro como solução profissional:

  Tentar o modelo `gemini-1.5-pro-latest`.
  O gemini-pro (versão 1.5) é muito mais "inteligente" que o Flash. Se o Flash está se
  confundindo e mandando argumentos vazios, o Pro tem muito mais chance de entender a
  instrução e mandar o JSON correto.

  Se o gemini-1.5-pro-latest também der erro 404, então o problema é definitivamente na sua
  API Key ou na biblioteca @ai-sdk/google que pode estar desatualizada, e aí teremos que
  investigar isso.

  Posso tentar mudar para o `gemini-1.5-pro-latest`? É a tentativa mais profissional antes
  de partirmos para "gambiarras".